(window.webpackJsonp=window.webpackJsonp||[]).push([[168],{541:function(v,a,_){"use strict";_.r(a);var e=_(7),p=Object(e.a)({},(function(){var v=this,a=v._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":v.$parent.slotKey}},[a("h2",{attrs:{id:"前言"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#前言"}},[v._v("#")]),v._v(" 前言")]),v._v(" "),a("p",[v._v("大数据相关组件之间的关系是相互依赖和相互配合的，它们共同构成了大数据处理和分析的生态系统。")]),v._v(" "),a("h2",{attrs:{id:"数据存储组件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#数据存储组件"}},[v._v("#")]),v._v(" 数据存储组件")]),v._v(" "),a("p",[v._v("Hadoop分布式文件系统（HDFS）：")]),v._v(" "),a("p",[v._v("作用：HDFS是Hadoop的核心组件之一，用于存储大规模数据集。它将数据划分成块并分布在多个节点上，提供了高度容错性和可扩展性。")]),v._v(" "),a("p",[v._v("关系：HDFS是大数据处理的基础，为其他组件如MapReduce、Spark等提供数据存储服务。")]),v._v(" "),a("h2",{attrs:{id:"数据处理组件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#数据处理组件"}},[v._v("#")]),v._v(" 数据处理组件")]),v._v(" "),a("p",[v._v("存下数据之后，就开始考虑怎么处理数据。虽然HDFS可以为你整体管理不同机器上的数据，但是这些数据太大了。如果单机处理时间不可想象，所以需要多机处理，那么就面临着一个问题，怎么分配工作？这就是Mapreduce的诞生的原因。")]),v._v(" "),a("p",[a("strong",[v._v("MapReduce：")])]),v._v(" "),a("p",[v._v("作用：MapReduce是Hadoop中用于处理和分析大规模数据集的编程模型。它将任务分解成Map和Reduce两个阶段，实现并行化处理，支持大规模数据的批处理。")]),v._v(" "),a("p",[v._v("关系：MapReduce与HDFS紧密配合，从HDFS中读取数据，处理后再将结果写回HDFS。同时，MapReduce也是Spark等新一代大数据处理框架的灵感来源。")]),v._v(" "),a("p",[v._v("MapReduce的设计，采用了很简化的计算模型，只有Map和Reduce两个计算过程（中间用Shuffle串联），用这个模型，已经可以处理大数据领域很大一部分问题了。那什么是Map（映射），什么是Reduce（规约）？")]),v._v(" "),a("p",[v._v("考虑如果你要统计一个巨大的文本文件存储在类似HDFS上，你想要知道这个文本里各个词的出现频率。你启动了一个MapReduce程序。")]),v._v(" "),a("p",[a("strong",[v._v("Map阶段：")])]),v._v(" "),a("p",[v._v("几百台机器同时读取这个文件的各个部分，分别把各自读到的部分分别统计出词频，产生类似（hello, 12100次），（world，15214次）等等这样的Pair（我这里把Map和Combine放在一起说以便简化）；这几百台机器各自都产生了如上的集合，然后又有几百台机器启动Reduce处理。")]),v._v(" "),a("p",[a("strong",[v._v("Reduce阶段：")])]),v._v(" "),a("p",[v._v("Reducer机器A将从Mapper机器收到所有以A开头的统计结果，机器B将收到B开头的词汇统计结果（当然实际上不会真的以字母开头做依据，而是用函数产生Hash值以避免数据串化。因为类似X开头的词肯定比其他要少得多，而你不希望数据处理各个机器的工作量相差悬殊）。然后这些Reducer将再次汇总：（hello，12100）＋（hello，12311）＋（hello，345881）= （hello，370292）。每个Reducer都如上处理，你就得到了整个文件的词频结果。")]),v._v(" "),a("p",[a("strong",[v._v("Spark：")])]),v._v(" "),a("p",[v._v("作用：Spark是一个基于内存计算的大数据处理框架，提供了比MapReduce更快的处理速度。提高了在大数据环境下数据处理的实时性，同时保证了高容错性和高可伸缩性，允许用户将Spark部署在大量廉价硬件之上，形成集群。Spark包含多个组件，如Spark Core、Spark SQL、Spark Streaming等。")]),v._v(" "),a("p",[v._v("关系：Spark可以运行在HDFS之上，利用HDFS的存储能力进行数据处理。同时，Spark的各个组件之间也相互协作，共同提供强大的数据处理能力。")]),v._v(" "),a("p",[a("strong",[v._v("Spark Core：")]),v._v(" 提供任务调度、内存管理和容错机制等基础功能。")]),v._v(" "),a("p",[a("strong",[v._v("Spark SQL：")]),v._v(" 用于处理结构化数据，提供SQL查询接口。")]),v._v(" "),a("p",[a("strong",[v._v("Spark Streaming：")]),v._v(" 用于实时数据处理和分析。")]),v._v(" "),a("p",[a("strong",[v._v("Spark MLlib：")]),v._v(" 提供机器学习算法和工具。")]),v._v(" "),a("p",[a("strong",[v._v("Apache Flink：")])]),v._v(" "),a("p",[v._v("作用：一种流处理框架，支持事件驱动和精确一次语义。Flink能够处理无界和有界的数据流，为实时数据分析和处理提供了强大支持。")]),v._v(" "),a("p",[v._v("关系：Flink与Spark Streaming等组件在实时数据处理领域存在竞争关系，但各自具有不同的特点和优势。")]),v._v(" "),a("h2",{attrs:{id:"数据仓库与查询组件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#数据仓库与查询组件"}},[v._v("#")]),v._v(" 数据仓库与查询组件")]),v._v(" "),a("p",[v._v("有了MapReduce，Tez和Spark之后，程序员发现，MapReduce的程序写起来真麻烦，他们希望简化这个过程。于是就有了Hive。")]),v._v(" "),a("p",[a("strong",[v._v("Apache Hive：")])]),v._v(" "),a("p",[v._v("Hive基于hadoop的数据仓库工作，可以将结构性的数据映射成一张数据库表，Hive用的是SQL。将SQL语言翻译成MapReduce程序，丢给计算引擎去计算，这样大家只需要用更简单更直观的语言去写程序了。总之就是简化MapReduce的编程过程。")]),v._v(" "),a("p",[v._v("作用：基于Hadoop的数据仓库工具，提供了类似SQL的查询语言（HiveQL），使得用户能够在Hadoop集群上执行数据分析。")]),v._v(" "),a("p",[v._v("关系：Hive建立在HDFS和MapReduce之上，通过HiveQL查询语言简化大数据查询过程。同时，Hive也支持与其他大数据组件如Spark的集成。")]),v._v(" "),a("p",[a("strong",[v._v("Presto、Drill等：")])]),v._v(" "),a("p",[v._v("作用：这些组件是分布式SQL查询引擎，用于查询分布在一个或多个不同数据源中的大数据集。")]),v._v(" "),a("p",[v._v("关系：它们与Hive类似，但可能具有更快的查询速度和更优化的查询性能。这些组件通常与HDFS等存储组件配合使用。")]),v._v(" "),a("h2",{attrs:{id:"消息队列与数据流组件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#消息队列与数据流组件"}},[v._v("#")]),v._v(" 消息队列与数据流组件")]),v._v(" "),a("p",[a("strong",[v._v("Apache Kafka：")])]),v._v(" "),a("p",[v._v("作用：一个高吞吐量的分布式消息队列系统，用于实时数据传输。Kafka能够持久化、分发和处理流式数据。")]),v._v(" "),a("p",[v._v("关系：Kafka在大数据生态系统中扮演着数据管道的角色，将数据源产生的数据实时传输到处理组件如Spark Streaming、Flink等进行处理。")]),v._v(" "),a("p",[a("strong",[v._v("Apache NiFi：")])]),v._v(" "),a("p",[v._v("作用：一个用于构建数据流的开源数据集成工具，支持数据采集、传输和处理。")]),v._v(" "),a("p",[v._v("关系：NiFi提供了直观的图形化界面，使得用户能够轻松设计和管理数据流。它与其他大数据组件如Kafka、HDFS等配合使用，实现数据流的自动化处理。")]),v._v(" "),a("h2",{attrs:{id:"其他组件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#其他组件"}},[v._v("#")]),v._v(" 其他组件")]),v._v(" "),a("p",[a("strong",[v._v("BI数据分析工具：")])]),v._v(" "),a("p",[v._v("作用：BI工具是一类专门设计用于帮助企业收集、分析和可视化数据的软件工具。它们将庞大、复杂的数据集转化为直观、易于理解的图形和报表。")]),v._v(" "),a("p",[v._v("关系：BI工具通常与大数据处理组件配合使用，将处理后的数据以可视化的形式呈现给用户，帮助用户更好地理解业务状况并做出决策。")]),v._v(" "),a("p",[v._v("综上所述，大数据相关组件之间的关系是相互依赖和相互配合的。它们通过各自的功能和特性共同构成了大数据处理和分析的生态系统，为企业提供了从数据存储、处理到分析的全方位解决方案。在这个生态系统中，每个组件都扮演着不可或缺的角色，共同推动着大数据技术的不断发展和创新。")])])}),[],!1,null,null,null);a.default=p.exports}}]);